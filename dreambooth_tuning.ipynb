{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48648f2c-2ef8-41b1-b5c6-3b0f655b1005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"downloaded.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"train_data\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ec3badf-bd4c-44ac-bc7c-8a60b070dbef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "files = glob.glob(\"train_data/*/*/*.webp\")\n",
    "os.makedirs(\"train1\", exist_ok=True)\n",
    "c = 0\n",
    "\n",
    "for f in files:\n",
    "    shutil.copy2(f, f\"train1/{c}_{f.split('/')[-1]}\")\n",
    "    c += 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60979118-cbd7-427c-923d-3f450c11e4ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "!pip install git+https://github.com/huggingface/diffusers\n",
    "!pip install -U -r requirements.txt \\\n",
    "  --user\n",
    "!pip install xformers\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa629a6-5e9b-416c-8e94-efee99bc99fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!huggingface-cli login --token \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ae0dba-4455-4041-8ff1-53b79f4e3277",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /home/jupyter-alexsmirnov6/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    }
   ],
   "source": [
    "!accelerate config default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81c78ce8-05fd-459d-9cc4-cb13acde64bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd792413-c017-40cb-879d-64799aa25c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(1)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d61ae3-f0d9-4202-8688-3652eee8cafe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "/opt/tljh/user/lib/python3.9/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of ðŸ¤— Accelerate. Use `project_dir` instead.\n",
      "  warnings.warn(\n",
      "05/02/2023 17:33:35 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'sample_max_value', 'variance_type', 'thresholding', 'dynamic_thresholding_ratio', 'prediction_type', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
      "{'norm_num_groups'} was not found in config. Values will be initialized to default values.\n",
      "{'upcast_attention', 'addition_embed_type_num_heads', 'resnet_skip_time_act', 'conv_in_kernel', 'use_linear_projection', 'addition_embed_type', 'time_embedding_type', 'conv_out_kernel', 'time_embedding_dim', 'mid_block_only_cross_attention', 'resnet_out_scale_factor', 'dual_cross_attention', 'cross_attention_norm', 'class_embed_type', 'mid_block_type', 'class_embeddings_concat', 'projection_class_embeddings_input_dim', 'time_embedding_act_fn', 'num_class_embeds', 'resnet_time_scale_shift', 'timestep_post_act', 'only_cross_attention', 'encoder_hid_dim', 'time_cond_proj_dim'} was not found in config. Values will be initialized to default values.\n",
      "05/02/2023 17:33:42 - INFO - __main__ - ***** Running training *****\n",
      "05/02/2023 17:33:42 - INFO - __main__ -   Num examples = 7744\n",
      "05/02/2023 17:33:42 - INFO - __main__ -   Num batches each epoch = 484\n",
      "05/02/2023 17:33:42 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "05/02/2023 17:33:42 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "05/02/2023 17:33:42 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "05/02/2023 17:33:42 - INFO - __main__ -   Total optimization steps = 484\n",
      "Steps: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 484/484 [05:19<00:00,  1.75it/s, loss=0.204, lr=5e-6]{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
      "/opt/tljh/user/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
      "{'norm_num_groups'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in model1/vae/config.json\n",
      "Model weights saved in model1/vae/diffusion_pytorch_model.bin\n",
      "Configuration saved in model1/unet/config.json\n",
      "Model weights saved in model1/unet/diffusion_pytorch_model.bin\n",
      "Configuration saved in model1/scheduler/scheduler_config.json\n",
      "Configuration saved in model1/model_index.json\n",
      "Steps: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 484/484 [05:52<00:00,  1.37it/s, loss=0.204, lr=5e-6]\n",
      "EPOCH 2\n",
      "/opt/tljh/user/lib/python3.9/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of ðŸ¤— Accelerate. Use `project_dir` instead.\n",
      "  warnings.warn(\n",
      "05/02/2023 17:39:43 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'variance_type', 'clip_sample_range', 'sample_max_value', 'dynamic_thresholding_ratio', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "05/02/2023 17:39:48 - INFO - __main__ - ***** Running training *****\n",
      "05/02/2023 17:39:48 - INFO - __main__ -   Num examples = 7744\n",
      "05/02/2023 17:39:48 - INFO - __main__ -   Num batches each epoch = 484\n",
      "05/02/2023 17:39:48 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "05/02/2023 17:39:48 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "05/02/2023 17:39:48 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "05/02/2023 17:39:48 - INFO - __main__ -   Total optimization steps = 484\n",
      "Steps: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 484/484 [04:57<00:00,  1.63it/s, loss=0.179, lr=5e-6]/opt/tljh/user/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Configuration saved in model1/vae/config.json\n",
      "Model weights saved in model1/vae/diffusion_pytorch_model.bin\n",
      "Configuration saved in model1/unet/config.json\n",
      "Model weights saved in model1/unet/diffusion_pytorch_model.bin\n",
      "Configuration saved in model1/scheduler/scheduler_config.json\n",
      "Configuration saved in model1/model_index.json\n",
      "Steps: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 484/484 [05:33<00:00,  1.45it/s, loss=0.179, lr=5e-6]\n",
      "EPOCH 3\n",
      "/opt/tljh/user/lib/python3.9/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of ðŸ¤— Accelerate. Use `project_dir` instead.\n",
      "  warnings.warn(\n",
      "05/02/2023 17:45:36 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'clip_sample_range', 'sample_max_value', 'variance_type', 'thresholding', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n",
      "05/02/2023 17:45:48 - INFO - __main__ - ***** Running training *****\n",
      "05/02/2023 17:45:48 - INFO - __main__ -   Num examples = 7744\n",
      "05/02/2023 17:45:48 - INFO - __main__ -   Num batches each epoch = 484\n",
      "05/02/2023 17:45:48 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "05/02/2023 17:45:48 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "05/02/2023 17:45:48 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "05/02/2023 17:45:48 - INFO - __main__ -   Total optimization steps = 484\n",
      "Steps:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 196/484 [02:19<03:30,  1.37it/s, loss=0.21, lr=5e-6]"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "print('EPOCH 1')\n",
    "!accelerate launch train_model.py \\\n",
    "  --pretrained_model_name_or_path=\"CompVis/stable-diffusion-v1-4\"  \\\n",
    "  --instance_data_dir=\"train1\" \\\n",
    "  --class_data_dir=\"images1\" \\\n",
    "  --output_dir=\"model1\" \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --instance_prompt=\"a sks sticker on a white background\" \\\n",
    "  --class_prompt=\"a sticker on a white background\" \\\n",
    "  --resolution=128 \\\n",
    "  --train_batch_size=16 \\\n",
    "  --sample_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=1 --gradient_checkpointing \\\n",
    "  --learning_rate=5e-6 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --num_class_images=15 \\\n",
    "  #--max_train_steps=800 \\\n",
    "  --mixed_precision=fp16 \n",
    "\n",
    "for epoch in range(2, num_epochs+1):\n",
    "    print(f'EPOCH {epoch}')\n",
    "    \n",
    "    !accelerate launch train_model.py \\\n",
    "      --pretrained_model_name_or_path=\"model1\"  \\\n",
    "      --instance_data_dir=\"train1\" \\\n",
    "      --class_data_dir=\"images1\" \\\n",
    "      --output_dir=\"model1\" \\\n",
    "      --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "      --instance_prompt=\"a sks sticker on a white background\" \\\n",
    "      --class_prompt=\"a sticker on a white background\" \\\n",
    "      --resolution=128 \\\n",
    "      --train_batch_size=16 \\\n",
    "      --sample_batch_size=1 \\\n",
    "      --gradient_accumulation_steps=1 --gradient_checkpointing \\\n",
    "      --learning_rate=5e-6 \\\n",
    "      --lr_scheduler=\"constant\" \\\n",
    "      --lr_warmup_steps=0 \\\n",
    "      --num_class_images=15 \\\n",
    "      #--max_train_steps=800 \\\n",
    "      --mixed_precision=fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a684b5f5-ba5f-4117-9161-a53c2ee2f9be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "model_id = \"model1\"\n",
    "pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ced56e9e-30d0-45eb-886c-4a0baabd4d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a90b981a45c544b08d0abcc9ca184bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"a cartoon human sticker on a white background\"\n",
    "image = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
    "\n",
    "image.save(\"inference-example.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d9e40e-0d4d-43d0-be1c-ace2b70695e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate launch train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=\"model1\"  \\\n",
    "  --instance_data_dir=\"train_data/downloaded/adrianochelentano\" \\\n",
    "  --output_dir=\"result\" \\\n",
    "  --instance_prompt=\"a cartoon men sticker on a white background\" \\\n",
    "  --resolution=128 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=5e-4 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32bc7b3-6f5f-4af1-8579-0a534037dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "prompt = \"a cartoon men sticker on a white background\"\n",
    "image = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
    "\n",
    "image.save(\"inference-example.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
